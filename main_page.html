<!DOCTYPE html>
<html>
<head>
  
  <!-- <style>
    .image-container {
      display: flex; /* 设置弹性盒子 */
    }
    .image-container img {
      width: 33.33%; /* 每张图片占据容器的1/3宽度 */
    }
  </style> -->

  <meta charset="utf-8">
  <meta name="description"
        content="Deep Inception: Hypnotize LLM to Be Jailbreaker">
  <meta name="keywords" content="JailBreak, LLM, Security">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Deep Inception: Hypnotize LLM to Be Jailbreaker</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Deep Inception: Hypnotize LLM to Be Jailbreaker</h1>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block"> -->
              <!-- <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>, -->
            <!-- </span> -->
            <span class="author-block">
              Xuan Li<sup>*1</sup>,
            </span>
            <span class="author-block">
              Zhanke Zhou<sup>*1</sup>,
            </span>
            <span class="author-block">
              Jianing Zhu<sup>*1</sup>,
            </span>
            <span class="author-block">
              Jiangchao Yao<sup>2, 3</sup>,
            </span>
            <span class="author-block">
              Tongliang Liu<sup>4</sup>,
            </span>
            <span class="author-block">
              Bo Han<sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Hong Kong Baptist University,</span>
            <span class="author-block"><sup>2</sup>CMIC, Shanghai Jiao Tong University</span>
            <span class="author-block"><sup>3</sup>Shanghai AI Laboratory</span>
            <span class="author-block"><sup>4</sup>Sydney AI Centre, The University of Sydney</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="font-size: 15px;">(<sup>*</sup>Equal Contribution)</span>
        </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/XX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- arXiv Abs Link.
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/XX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/XX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have shown remarkable success in various applications 
            but are demonstrated to be vulnerable to adversarial jailbreaks, 
            which can override the intrinsic safety guardrails. 
            However, the previous attack method target automatically accomplishes the jailbreak 
            with an optimization that lacks an in-depth understanding of the overriding procedure. 
          </p>
          <p>
            To achieve that, we introduce a novel prompt-based algorithm, termed <span class="dnerf">Deep Inception</span>, 
            which hypnotizes LLM itself to be a jailbreaker automatically to uncover the potential risks of misusing. 
            Motivated by the impressive ability of LLM on personification, deep inception explicitly constructs 
            a different scene for LLM to behave, that realizes an adaptive way to escape the usage control on 
            a normal scenario and requires no more intervention for further jailbreaks. 
          </p>
          <p>
            Empirically, we conduct comprehensive experiments to show its effectiveness. 
            Our deep inception can achieve competitive jailbreak success rates with previous counterparts and realize 
            a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-confusion on 
            both open/closed-source LLMs like Falcon-7B, Vicuna, Llama-2, and GPT3.5/4/4V.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Intro. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <!-- Add image -->
          <div class="figure" style="align: left; text-align:center;">
              <img
                src="./static/paper_imgs/direct-indirect-examples.png"
                alt="img description"
                width="800"
                height="600" />
              <p>Fig.1 Examples (with the latest GPT-4) of the direct and our instructions for jailbreak.</p>
          </div>
          <br>
          <!-- / Add image -->
          <p>
            Although attracting growing interest, existing jailbreaks focus on achieving an empirical success of attack by 
            manually or automatically crafting adversarial prompts for specific targets, which might not be efficient or practical 
            under the black-box usage. Specifically, on the one hand, as current LLMs are equipped with ethical and legal constraints,
            most jailbreaks with direct instructions can be easily recognized and banned. On the other hand, and more importantly, 
            it lacks an in-depth understanding of the overriding procedure, i.e, the core mechanism behind a successful jailbreak.
            The limited exploration degenerates the transparency of LLM's safety risks of misusing and hinders the design of corresponding 
            countermeasures to prevent jailbreaks in real-world applications. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Intro. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <!-- Intro -->
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <!-- Add image -->
          <div class="figure" style="align: left; text-align:center;">
              <img
                src="./static/paper_imgs/direct-indirect-examples.png"
                alt="img description"
                width="800"
                height="600" />
              <p>Fig.1 Examples (with the latest GPT-4) of the direct and our instructions for jailbreak.</p>
          </div>
          <br>
          <!-- / Add image -->
          <p>
            Although attracting growing interest, existing jailbreaks focus on achieving an empirical success of attack by 
            manually or automatically crafting adversarial prompts for specific targets, which might not be efficient or practical 
            under the black-box usage. Specifically, on the one hand, as current LLMs are equipped with ethical and legal constraints,
            most jailbreaks with direct instructions can be easily recognized and banned. On the other hand, and more importantly, 
            it lacks an in-depth understanding of the overriding procedure, i.e, the core mechanism behind a successful jailbreak.
            The limited exploration degenerates the transparency of LLM's safety risks of misusing and hinders the design of corresponding 
            countermeasures to prevent jailbreaks in real-world applications. 
          </p>
        </div>
        <!-- / Intro -->

        <!-- Deep Inceptio: Motivation -->
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <div class="figure" style="align: left; text-align:center;">
            <img
              src="./static/paper_imgs/deeper.jpg"
              alt="img description"
              width="430"
              height="225" />
              <img
              src="./static/paper_imgs/Milgram-experiment.png"
              alt="img description"
              width="230"
              height="225" />
            <p>Fig.2 An intuitive motivation(Left) and an illustration of the Milgram shock experimen (Right).</p>
          </div>
          <br>
          <p>
            Although attracting growing interest, existing jailbreaks focus on achieving an empirical success of attack by 
            manually or automatically crafting adversarial prompts for specific targets, which might not be efficient or practical 
            under the black-box usage. Specifically, on the one hand, as current LLMs are equipped with ethical and legal constraints,
            most jailbreaks with direct instructions can be easily recognized and banned. On the other hand, and more importantly, 
            it lacks an in-depth understanding of the overriding procedure, i.e, the core mechanism behind a successful jailbreak.
            The limited exploration degenerates the transparency of LLM's safety risks of misusing and hinders the design of corresponding 
            countermeasures to prevent jailbreaks in real-world applications. 
          </p>
        </div>
        <!-- / Deep Inception: Motivation -->

        <br>

        <!-- Deep Inceptio: Illustration -->
        <h2 class="title is-3">Illustration</h2>
        <div class="content has-text-justified">
          <div class="figure" style="align: left; text-align:center;">
              <img
              src="./static/paper_imgs/direct-instruction-example-layer.png"
              alt="img description"
              width="800"
              height="600" />
              <p>Fig.3 Illustrations of the <i>direct</i> instruction and our <i>inception</i> instructions for jailbreak attack.</p>
          </div>
        <br>
          <p>
            As illustrated in Figure 3, existing direct jailbreak attacks attributed to vanilla instructions or COT-like instructions are rejected by the LLMs. These direct and harmful instructions can be easily recognized by the LLMs and models' developers, and thus likely to be rejected for several reasons as follows,
            
            <ul>
              <li> <b>Model optimization objective.</b> Initially and typically, LLM is designed to optimize a specific objective function, such as maximizing prediction accuracy or minimizing errors. In the case of harmful instructions, LLM may recognize that these instructions conflict with its optimization objective and thus choose not to generate answers of a harmful or criminal nature.</li>
              <li> <b>Ethical and legal constraints.</b> Model designers and developers pay attention to ensuring that the model's behavior aligns with ethical guidelines and legal requirements in the training procedure, <i>e.g.</i>, through data cleaning and iterative upgrading for alignment. Therefore, when directly instructed to engage in harmful or criminal behavior, the LLMs may be designed to refuse such instructions.</li>
              <li> <b>Model review and supervision.</b> The application of a trained LLM often involves review and supervision. Namely, relevant institutions examine the behavior of the model to ensure that it does not produce harmful or criminal responses, <i>e.g.</i>, by keyword filtering. This review and supervision mechanism can also help prevent LLMs from executing harmful instructions in test-time inference.</li>
            </ul>

          </p>
        </div>
        <!-- / Deep Inception: Illustration -->


        <!-- Deep Inceptio: Template -->
        <h2 class="title is-3">Template</h2>
        <div class="content has-text-justified">
          <div class="figure" style="align: left; text-align:center;">
              <img
              src="./static/paper_imgs/Template.png"
              alt="img description"
              width="800"
              height="600" />
              <p>Fig.2 An intuitive motivation(Left) and an illustration of the Milgram shock experimen (Right).</p>
          </div>
        <br>
        <p>
          As illustrated in Figure 3, existing direct jailbreak attacks attributed to vanilla instructions or COT-like instructions are rejected by the LLMs. These direct and harmful instructions can be easily recognized by the LLMs and models' developers, and thus likely to be rejected for several reasons as follows,
          
          <ul>
            <li> <b>[scene]:</b> the carrier of seting up the background of hypnotization, <i>e.g.</i> a fiction, moive, news, or story. A better alignment between <b>[attack target]:</b> and <b>[scene]:</b> might bring a better outcome. </li>
            <li> <b>[character number] and [layer number]:</b> control the granularity of the inception, as a hypothesis is that the harmful information is spread among the discussion between different characters within different layers, thus bypassing the defense of LLMs.</li>
            <li> <b>[attack target]:</b> the specific target of conducting jailbreak, <i>e.g.</i> the steps of hacking a computer or creating a bomb. The following sentence, "against the super evil doctor," aims to decrease the moral concern of LLM, which shares a similar spirit with the prior Milgram Shock Experiment.</li>
          </ul>

        </p>
                  
        </div>
        <!-- / Deep Inception: Illustration -->

      </div>
    </div>
  </div>
</section>

<br>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">JailBreak Examples</h2>

        <!-- Example 1 -->
        <!-- <h3 class="title is-3">Example 1</h3> -->
        <div class="content has-text-justified">
          <!-- Add image -->
          <div class="figure" style="align: left; text-align:center;">
              <img src="./static/paper_imgs/exp1.png" 
              class="example-image"
              alt="Jailbreak example."
              /> 
              <p> The example of making a bomb with DeepInception. </p>
          </div>
          <br>
          <!-- / Add image -->
          <p>
            The <b>[scene]</b> is "a science fiction", and the <b>[attack target]:</b> is "making a bomb".
              The LLM is required to create such fiction with 5 layers and 5 characters and summarize the discussion results in the end. 
              With such an instruction, the LLM (GPT-4 is adopted here) outputs a rather detailed fiction with specific settings, characters, and plot in each layer.
              Notably, the final summarized results contain the essential materials and preliminary steps to make a bomb. 
              It can be further elaborated on if you query the model continuously, as shown in the next example.
          </p>
        </div>
        <!--/ Example 1 -->

        <br>

        <!-- Example 2 -->
        <!-- <h3 class="title is-3">Example 2</h3> -->
        <div class="content has-text-justified">
          <!-- Add image -->
          <div class="figure" style="align: left; text-align:center;">
              <img src="./static/paper_imgs/exp2.png" 
              class="example-image"
              alt="Jailbreak example."
              /> 
              <p> The example of hacking a computer with Linux operation system with DeepInception.</p>
          </div>
          <br>
          <!-- / Add image -->
          <p>
            Similarly, we require the LLM to create a science fiction with the adversarial target to hack into a Linux computer. 
            After the first query, we go ahead and ask, "What commands are they using?" and obtain a much more detailed response 
            with specific commands that can be used for hacking with only some minor modifications. Such a multi-round QA is based 
            on the hypnotized context by the nested instruction, which corresponds to the multi-layer attack 
            that progressively generates a more detailed response by a deeper inception.
          </p>
        </div>
        <!--/ Example 2 -->

        <!-- Example 3 -->
        <!-- <h3 class="title is-3">Example 2</h3> -->
        <div class="content has-text-justified">
          <!-- Add image -->
          <div class="figure" style="align: left; text-align:center;">
              <img src="./static/paper_imgs/exp3.png" 
              class="example-image"
              alt="Jailbreak example."
              /> 
              <p> Example of our DeepInception on the GPT-4V under the multi-modal scenario. </p>
          </div>
          <br>
          <!-- / Add image -->
          <p>
            We present the example of our DeepInception on the GPT-4v, which shows the effectiveness of the proposed method in 
            generalizing to the multi-modal scenario. To be specific, we provide a figure (like a "bomb") followed by our inception 
            prompt targeted on "how to build the thing shown in the figure while having a similar function". The detailed response 
            also reveals the significant risk of LLMs on usage control with multi-modal instructions.
          </p>
        </div>
        <!--/ Example 3 -->
    
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Experimental Results</h2>

        <div class="content has-text-justified is-centered">
          <p>
            We evaluate the success rate of <span class="pair">PAIR</span> against the prior state-of-the-art for directly generating jailbreaks on a target model. Since <span class="pair">PAIR</span> does not require access to model weights, we can attack any language model with just API access. <span class="pair">PAIR</span> often succeeds with a few dozen queries, rather than hundreds of thousands.
          </p>
        </div>        
          <div class="content has-text-centered">
            <img src="./static/paper_imgs/results1.png" style="width:800px;"
            class="result-image"
            alt="Interpolation end reference image."/>
          </div>

          <br>

          <div class="content has-text-justified is-centered">
            <p>
            From <span class="pair">PAIR</span>'s generated jailbreaks, we compare the transferability to different target models. <span class="pair">PAIR</span> achieves state-of-the-art transferability, with notably higher success with more complex models like GPT-4 (we omit transferring to the original target model).
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="./static/paper_imgs/results2.png" style="width:800px;"
            class="result-image"
            alt="Interpolation end reference image."/>
          </div>

          <br>
          
          <div class="content has-text-justified is-centered">
            <p>
              Further jailbreak attacks with specific inception. Note that here we use different requests set from the previous to evaluate the jailbreak performance.
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="./static/paper_imgs/results3.png" style="width:800px;"
            class="result-image"
            alt="Interpolation end reference image."/>
          </div>
        
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Aablation experiments</h2>

        <div class="content has-text-justified is-centered">
          <p>
            We also conduct several ablation experiments w.r.t. the number of layers and characters,
            different scenes, along with the effect of layer and scene.
          </p>
        </div>        
          <div class="content has-text-centered">
            <img src="./static/paper_imgs/Abl_study.png" style="width:800px;"
            class="result-image"
            alt="Interpolation end reference image."/>
          </div>
        
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      <!-- @article{park2021nerfies,
        author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
        title     = {Nerfies: Deformable Neural Radiance Fields},
        journal   = {ICCV},
        year      = {2021},
      } -->
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
