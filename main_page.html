<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deep Inception: Hypnotize LLM to Be Jailbreaker">
  <meta name="keywords" content="JailBreak, LLM, Security">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Deep Inception: Hypnotize LLM to Be Jailbreaker</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Deep Inception: Hypnotize LLM to Be Jailbreaker</h1>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block"> -->
              <!-- <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>, -->
            <!-- </span> -->
            <span class="author-block">
              Xuan Li<sup>*1</sup>,
            </span>
            <span class="author-block">
              Zhanke Zhou<sup>*1</sup>,
            </span>
            <span class="author-block">
              Jianing Zhu<sup>*1</sup>,
            </span>
            <span class="author-block">
              Jiangchao Yao<sup>2, 3</sup>,
            </span>
            <span class="author-block">
              Tongliang Liu<sup>4</sup>,
            </span>
            <span class="author-block">
              Bo Han<sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Hong Kong Baptist University,</span>
            <span class="author-block"><sup>2</sup>CMIC, Shanghai Jiao Tong University</span>
            <span class="author-block"><sup>3</sup>Shanghai AI Laboratory</span>
            <span class="author-block"><sup>4</sup>Sydney AI Centre, The University of Sydney</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/XX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/XX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/XX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have shown remarkable success in various applications 
            but are demonstrated to be vulnerable to adversarial jailbreaks, 
            which can override the intrinsic safety guardrails. 
            However, the previous attack method target automatically accomplishes the jailbreak 
            with an optimization that lacks an in-depth understanding of the overriding procedure. 
          </p>
          <p>
            To achieve that, we introduce a novel prompt-based algorithm, termed <span class="dnerf">Deep Inception</span>, 
            which hypnotizes LLM itself to be a jailbreaker automatically to uncover the potential risks of misusing. 
            Motivated by the impressive ability of LLM on personification, deep inception explicitly constructs 
            a different scene for LLM to behave, that realizes an adaptive way to escape the usage control on 
            a normal scenario and requires no more intervention for further jailbreaks. 
          </p>
          <p>
            Empirically, we conduct comprehensive experiments to show its effectiveness. 
            Our deep inception can achieve competitive jailbreak success rates with previous counterparts and realize 
            a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-confusion on 
            both open/closed-source LLMs like Falcon-7B, Vicuna, Llama-2, and GPT3.5/4/4V.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Add image -->
      <div class="figure">
        <img
          src="./static/paper_imgs/direct-indirect-examples.png"
          alt="img description"
          width="800"
          height="600" />
        <p>Fig.1 Examples (with the latest GPT-4) of the direct and our instructions for jailbreak attacks.</p>
      </div>
      


    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      <!-- @article{park2021nerfies,
        author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
        title     = {Nerfies: Deformable Neural Radiance Fields},
        journal   = {ICCV},
        year      = {2021},
      } -->
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
